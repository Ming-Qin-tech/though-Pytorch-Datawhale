{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9ba0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:19.226560Z",
     "start_time": "2022-03-15T02:25:18.574329Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    #声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "    def forward(self, x):\n",
    "        o = self.act(self.hidden(x))\n",
    "        return self.output(o)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987fa6c",
   "metadata": {},
   "source": [
    "**以上的 MLP 类中⽆须定义反向传播函数。系统将通过⾃动求梯度⽽自动⽣成反向传播所需的 backward 函数。**\n",
    "我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP **继承自 Module 类的 __call__ 函数**，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2fcefd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:19.273256Z",
     "start_time": "2022-03-15T02:25:19.261801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeaab603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:19.468958Z",
     "start_time": "2022-03-15T02:25:19.451445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1442,  0.0418,  0.0844, -0.1390, -0.2477,  0.0780, -0.0380,  0.1658,\n",
       "          0.0950,  0.1681],\n",
       "        [ 0.2690,  0.0787,  0.1163, -0.1810, -0.2572,  0.0595, -0.0413,  0.0304,\n",
       "         -0.0462,  0.1841]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173de84",
   "metadata": {},
   "source": [
    "### 自定义模型层\n",
    "\n",
    "- 不含模型参数的层\n",
    "    - 下⾯构造的 MyLayer 类通过继承 Module 类自定义了一个**将输入减掉均值后输出**的层，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0a6261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:19.798505Z",
     "start_time": "2022-03-15T02:25:19.791840Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e459314b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:19.988364Z",
     "start_time": "2022-03-15T02:25:19.983351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLayer()\n"
     ]
    }
   ],
   "source": [
    "layer = MyLayer()\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b2424d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:25:20.289108Z",
     "start_time": "2022-03-15T02:25:20.280563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.tensor([1,2,3,4,5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34493785",
   "metadata": {},
   "source": [
    "#### 含模型参数的层\n",
    "我们还可以自定义含模型参数的自定义层。**其中的模型参数可以通过训练学出。**\n",
    "\n",
    "- Parameter类其实是Tensor的子类,如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。\n",
    "    - 所以在自定义含模型参数的层时，我们应该将参数定义成Parameter\n",
    "    - 除了直接定义成Parameter类外，还可以使用ParameterList和parameterDict分别定义参数的列表和字典。\n",
    "\n",
    "##### 两种定义模型参数的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b619537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:26:50.331874Z",
     "start_time": "2022-03-15T02:26:50.320236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "    \n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d6994f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:47:06.607664Z",
     "start_time": "2022-03-15T02:47:06.595337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (Linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (Linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (Linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'Linear1':nn.Parameter(torch.randn(4,4)),\n",
    "                'Linear2':nn.Parameter(torch.randn(4,1))\n",
    "        })\n",
    "        self.params.update({'Linear3': nn.Parameter(torch.randn(4, 2))})  #新增\n",
    "    \n",
    "    def forward(self, x, choice='Linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "    \n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2964227",
   "metadata": {},
   "source": [
    "#### 定义一些常见的卷积、池化层\n",
    "#### 二维卷积层\n",
    "- 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。\n",
    "    - 卷积层的模型参数包括了**卷积核**和**标量偏差**。\n",
    "    - 在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c7c3261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T02:56:56.701732Z",
     "start_time": "2022-03-15T02:56:56.688243Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 卷积运算（二维互相关）\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515a0633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T03:10:33.058537Z",
     "start_time": "2022-03-15T03:10:33.052631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D()\n"
     ]
    }
   ],
   "source": [
    "net = Conv2D((3,3))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418d9d5",
   "metadata": {},
   "source": [
    "- 填充(padding)是指在输⼊高和宽的两侧填充元素(通常是0元素)。\n",
    "\n",
    "    - 下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。\n",
    "    - 给定一 个高和宽为8的输入，我们发现输出的高和宽也是8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78e4a650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T03:10:37.753173Z",
     "start_time": "2022-03-15T03:10:37.739936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    #(1, 1)代表批量大小和通道数\n",
    "    X = X.view((1, 1) + X.shape)    # view函数的意思是numpy中的reshape\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:]) # 排除不关心的前两维：批量和通道\n",
    "\n",
    "# 注意这里是两侧分别填充1行或列，所以在两侧一共填充2行或列\n",
    "# 这里的Conv2d是pytorch集成的，而非上面自定义的Conv2d\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "X = torch.rand(8,8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3471b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mtorch]",
   "language": "python",
   "name": "conda-env-mtorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
